{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb368b52-270c-4190-8ded-72db59329485",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "initial settings"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Requiere: yfinance instalado en el cluster\n",
    "# Widgets\n",
    "dbutils.widgets.text(\"catalog\", \"ptd_dev\")\n",
    "dbutils.widgets.text(\"schema_bronze\", \"bronze\")\n",
    "dbutils.widgets.text(\"extra_tickers\", \"SPY,^VIX\")  # comma-separated\n",
    "dbutils.widgets.text(\"vendor_map_table\", \"\")       # opcional: ptd_dev.bronze.ticker_vendor_map\n",
    "\n",
    "from datetime import timedelta, date\n",
    "row = spark.sql(\"select max(date) as maxd from ptd_dev.bronze.prices_raw\").collect()[0]\n",
    "maxd = row.maxd  # tipo date o None\n",
    "if maxd:\n",
    "    start_date = maxd.isoformat()  # colchón por ajustes/splits\n",
    "else:\n",
    "    start_date = (now_utc - timedelta(days=1)).date().isoformat()\n",
    "\n",
    "# end_date: si tu job corre después del cierre, hoy; si corre antes, usa ayer\n",
    "import datetime as dt\n",
    "now_utc = dt.datetime.utcnow()\n",
    "end_date = now_utc.date().isoformat()  # o (now_utc.date() - timedelta(days=1)).isoformat()\n",
    "\n",
    "catalog         = dbutils.widgets.get(\"catalog\")\n",
    "schema_bronze   = dbutils.widgets.get(\"schema_bronze\")\n",
    "start_date      = dbutils.widgets.get(\"start_date\")\n",
    "end_date        = dbutils.widgets.get(\"end_date\") or None\n",
    "extra_tickers   = [t.strip().upper() for t in dbutils.widgets.get(\"extra_tickers\").split(\",\") if t.strip()]\n",
    "vendor_map_tbl  = dbutils.widgets.get(\"vendor_map_table\").strip()\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_bronze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97460afb-725a-4676-9167-77244ecce955",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "list tickers and import yfinance"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Instalá yfinance en el cluster (Libraries o %pip install yfinance).\") from e\n",
    "\n",
    "# 1) Lista de tickers desde el universo + extras\n",
    "tickers_df = spark.sql(f\"SELECT DISTINCT UPPER(ticker) AS ticker FROM {catalog}.bronze.universe_sp100_snapshot\")\n",
    "tickers = [r[\"ticker\"] for r in tickers_df.collect()]\n",
    "for t in extra_tickers:\n",
    "    if t not in tickers:\n",
    "        tickers.append(t)\n",
    "\n",
    "from notebooks.common.vendor_mapping import to_yfinance_symbo\n",
    "yf_symbols = {t: to_yfinance_symbol(t) for t in tickers + [\"SPY\",\"^VIX\"]}\n",
    "\n",
    "def to_yf(sym: str) -> str:\n",
    "    return yf_symbols.get(sym, sym)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3560def0-8e81-49d4-8268-1da13b16f393",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "download prices from OHLCV"
    }
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "failures = []\n",
    "for i, tk in enumerate(tickers, 1):\n",
    "    yf_sym = to_yf(tk)\n",
    "    try:\n",
    "        hist = yf.download(yf_sym, start=start_date, end=end_date, progress=False, auto_adjust=False, interval=\"1d\")\n",
    "        if hist is None or hist.empty:\n",
    "            failures.append((tk, \"empty\"))\n",
    "            continue\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\",\n",
    "                             \"Close\": \"close\", \"Adj Close\": \"adj_close\", \"Volume\": \"volume\"}, inplace=True)\n",
    "        hist[\"ticker\"] = tk\n",
    "        hist[\"source\"] = \"yfinance\"\n",
    "        rows.append(hist[[\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"source\"]])\n",
    "    except Exception as e:\n",
    "        failures.append((tk, str(e)))\n",
    "    # Tranquilizar rate limiting\n",
    "    time.sleep(0.2)\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No se descargaron precios. Revisá conexión o yfinance.\")\n",
    "\n",
    "pdf = pd.concat(rows, ignore_index=True)\n",
    "# A Spark DF\n",
    "sdf = spark.createDataFrame(pdf.assign(ingestion_ts=pd.Timestamp.utcnow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7acfd4e0-ddc5-4d89-8e48-b0582b7bf3d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load data in bronze table"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.bronze.prices_raw (\n",
    "  ticker STRING,\n",
    "  date   DATE,\n",
    "  open   DOUBLE,\n",
    "  high   DOUBLE,\n",
    "  low    DOUBLE,\n",
    "  close  DOUBLE,\n",
    "  adj_close DOUBLE,\n",
    "  volume   DOUBLE,\n",
    "  source   STRING,\n",
    "  ingestion_ts TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (date)\n",
    "\"\"\")\n",
    "\n",
    "sdf.createOrReplaceTempView(\"tmp_prices_in\")\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {catalog}.bronze.prices_raw AS t\n",
    "USING (\n",
    "  SELECT DISTINCT ticker, CAST(date AS DATE) AS date, open, high, low, close, adj_close, volume, source, ingestion_ts\n",
    "  FROM tmp_prices_in\n",
    ") s\n",
    "ON t.ticker = s.ticker AND t.date = s.date\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Descargas fallidas: {len(failures)}\")\n",
    "if failures:\n",
    "    display(spark.createDataFrame(failures, schema=\"ticker string, error string\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-bronze_prices_yfinance.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
