{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97460afb-725a-4676-9167-77244ecce955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Requiere: yfinance instalado en el cluster\n",
    "# Widgets\n",
    "dbutils.widgets.text(\"catalog\", \"ptd_dev\")\n",
    "dbutils.widgets.text(\"schema_bronze\", \"bronze\")\n",
    "dbutils.widgets.text(\"start_date\", \"2015-01-01\")\n",
    "dbutils.widgets.text(\"end_date\", \"\")\n",
    "dbutils.widgets.text(\"extra_tickers\", \"SPY,^VIX\")  # comma-separated\n",
    "dbutils.widgets.text(\"vendor_map_table\", \"\")       # opcional: ptd_dev.bronze.ticker_vendor_map\n",
    "\n",
    "catalog         = dbutils.widgets.get(\"catalog\")\n",
    "schema_bronze   = dbutils.widgets.get(\"schema_bronze\")\n",
    "start_date      = dbutils.widgets.get(\"start_date\")\n",
    "end_date        = dbutils.widgets.get(\"end_date\") or None\n",
    "extra_tickers   = [t.strip().upper() for t in dbutils.widgets.get(\"extra_tickers\").split(\",\") if t.strip()]\n",
    "vendor_map_tbl  = dbutils.widgets.get(\"vendor_map_table\").strip()\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_bronze}\")\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Instalá yfinance en el cluster (Libraries o %pip install yfinance).\") from e\n",
    "\n",
    "# 1) Lista de tickers desde el universo + extras\n",
    "tickers_df = spark.sql(f\"SELECT DISTINCT UPPER(ticker) AS ticker FROM {catalog}.bronze.universe_sp100_snapshot\")\n",
    "tickers = [r[\"ticker\"] for r in tickers_df.collect()]\n",
    "for t in extra_tickers:\n",
    "    if t not in tickers:\n",
    "        tickers.append(t)\n",
    "\n",
    "# 2) Vendor map (opcional): si existe, aplicar (por ejemplo para BRK.B -> BRK-B)\n",
    "vendor_map = {}\n",
    "if vendor_map_tbl:\n",
    "    exists = len([r for r in spark.sql(f\"SHOW TABLES IN {catalog}.bronze\").collect() if r['tableName']==vendor_map_tbl.split(\".\")[-1]])>0\n",
    "    if exists:\n",
    "        vm = spark.sql(f\"SELECT UPPER(ticker) AS ticker, vendor_symbol FROM {vendor_map_tbl}\")\n",
    "        vendor_map = {r['ticker']: r['vendor_symbol'] for r in vm.collect()}\n",
    "\n",
    "def to_yf(sym: str) -> str:\n",
    "    return vendor_map.get(sym, sym if sym.startswith(\"^\") else sym.replace(\".\", \"-\"))\n",
    "\n",
    "# 3) Descargar OHLCV\n",
    "rows = []\n",
    "failures = []\n",
    "for i, tk in enumerate(tickers, 1):\n",
    "    yf_sym = to_yf(tk)\n",
    "    try:\n",
    "        hist = yf.download(yf_sym, start=start_date, end=end_date, progress=False, auto_adjust=False, interval=\"1d\")\n",
    "        if hist is None or hist.empty:\n",
    "            failures.append((tk, \"empty\"))\n",
    "            continue\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\",\n",
    "                             \"Close\": \"close\", \"Adj Close\": \"adj_close\", \"Volume\": \"volume\"}, inplace=True)\n",
    "        hist[\"ticker\"] = tk\n",
    "        hist[\"source\"] = \"yfinance\"\n",
    "        rows.append(hist[[\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"source\"]])\n",
    "    except Exception as e:\n",
    "        failures.append((tk, str(e)))\n",
    "    # Tranquilizar rate limiting\n",
    "    time.sleep(0.2)\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No se descargaron precios. Revisá conexión o yfinance.\")\n",
    "\n",
    "pdf = pd.concat(rows, ignore_index=True)\n",
    "# A Spark DF\n",
    "sdf = spark.createDataFrame(pdf.assign(ingestion_ts=pd.Timestamp.utcnow()))\n",
    "\n",
    "# 4) Crear tabla y MERGE (idempotente)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.bronze.prices_raw (\n",
    "  ticker STRING,\n",
    "  date   DATE,\n",
    "  open   DOUBLE,\n",
    "  high   DOUBLE,\n",
    "  low    DOUBLE,\n",
    "  close  DOUBLE,\n",
    "  adj_close DOUBLE,\n",
    "  volume   DOUBLE,\n",
    "  source   STRING,\n",
    "  ingestion_ts TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (date)\n",
    "\"\"\")\n",
    "\n",
    "sdf.createOrReplaceTempView(\"tmp_prices_in\")\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {catalog}.bronze.prices_raw AS t\n",
    "USING (\n",
    "  SELECT DISTINCT ticker, CAST(date AS DATE) AS date, open, high, low, close, adj_close, volume, source, ingestion_ts\n",
    "  FROM tmp_prices_in\n",
    ") s\n",
    "ON t.ticker = s.ticker AND t.date = s.date\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Descargas fallidas: {len(failures)}\")\n",
    "if failures:\n",
    "    display(spark.createDataFrame(failures, schema=\"ticker string, error string\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-create_bronze_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
