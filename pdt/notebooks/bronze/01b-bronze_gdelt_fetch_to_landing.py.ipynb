{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c8c103-8ea7-4794-a601-625678daaa80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Descarga GDELT GKG v2.1 (.gkg.csv.zip), descomprime y guarda como .csv.gz en landing.\n",
    "# Luego, tu Auto Loader (bronze_gdelt_autoloader.py) toma esos .csv.gz y los carga a bronze.gdelt_gkg_raw.\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "# Si los widgets vienen vac√≠os, fijar ventana por defecto\n",
    "now_utc = dt.datetime.utcnow()\n",
    "default_start = (now_utc - timedelta(days=1)).date().isoformat()\n",
    "default_end   = now_utc.date().isoformat()\n",
    "\n",
    "try:\n",
    "    last_ts = spark.sql(\"select max(date_gkg) from ptd_dev.bronze.gdelt_gkg_raw\").collect()[0][0]\n",
    "except Exception as e:\n",
    "    last_ts = None\n",
    "\n",
    "if last_ts:\n",
    "    start_date_s = (last_ts - timedelta(hours=1)).date().isoformat()\n",
    "else:\n",
    "    start_date_s = default_start\n",
    "end_date_s   = dbutils.widgets.get(\"end_date\") or default_end\n",
    "\n",
    "dbutils.widgets.text(\"landing_path\", \"/Volumes/ptd_dev/bronze/raw/gdelt_gkg/\")  # carpeta monitoreada por Auto Loader\n",
    "dbutils.widgets.text(\"only_missing\", \"true\")      # si true, salta archivos que ya existen\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import timezone, timedelta\n",
    "import io, gzip, os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "start_date = dt.datetime.fromisoformat(dbutils.widgets.get(\"start_date\"))\n",
    "end_date_s = dbutils.widgets.get(\"end_date\").strip()\n",
    "end_date = (dt.datetime.fromisoformat(end_date_s) if end_date_s else dt.datetime.utcnow()).replace(tzinfo=None)\n",
    "\n",
    "landing_path = dbutils.widgets.get(\"landing_path\").rstrip(\"/\")\n",
    "only_missing = dbutils.widgets.get(\"only_missing\").lower() == \"true\"\n",
    "\n",
    "# Convenciones DBFS/Volumes:\n",
    "# - Para ESCRIBIR archivos con Python use /dbfs prefix\n",
    "# - Para LEER con Spark, use la ruta sin /dbfs\n",
    "dbfs_landing = f\"/dbfs{landing_path}\"\n",
    "\n",
    "os.makedirs(dbfs_landing, exist_ok=True)\n",
    "\n",
    "BASE = \"http://data.gdeltproject.org/gdeltv2\"\n",
    "\n",
    "def iter_times_15min(start: dt.datetime, end: dt.datetime):\n",
    "    # GDELT publica en cortes de 15m: 00,15,30,45\n",
    "    cur = start.replace(minute=(start.minute//15)*15, second=0, microsecond=0)\n",
    "    while cur <= end:\n",
    "        yield cur\n",
    "        cur += timedelta(minutes=15)\n",
    "\n",
    "def gdelt_gkg_url(ts: dt.datetime) -> str:\n",
    "    tsstr = ts.strftime(\"%Y%m%d%H%M%S\")\n",
    "    return f\"{BASE}/{tsstr}.gkg.csv.zip\", tsstr\n",
    "\n",
    "downloaded, skipped, errors = 0, 0, []\n",
    "\n",
    "for ts in iter_times_15min(start_date, end_date):\n",
    "    url, tsstr = gdelt_gkg_url(ts)\n",
    "    out_gz = f\"{dbfs_landing}/{tsstr}.gkg.csv.gz\"\n",
    "\n",
    "    if only_missing and os.path.exists(out_gz):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            # No todos los cortes existen; omitir 404\n",
    "            if r.status_code != 404:\n",
    "                errors.append((tsstr, f\"HTTP {r.status_code}\"))\n",
    "            continue\n",
    "\n",
    "        with ZipFile(io.BytesIO(r.content)) as zf:\n",
    "            # Debe contener 1 CSV. Tomamos el primero .csv\n",
    "            members = [n for n in zf.namelist() if n.lower().endswith(\".csv\")]\n",
    "            if not members:\n",
    "                errors.append((tsstr, \"ZIP sin CSV\"))\n",
    "                continue\n",
    "\n",
    "            with zf.open(members[0]) as csv_in:\n",
    "                raw = csv_in.read()\n",
    "\n",
    "        # Guardar como .csv.gz para que Spark/Auto Loader lo lean nativamente\n",
    "        with gzip.open(out_gz, \"wb\") as gz_out:\n",
    "            gz_out.write(raw)\n",
    "\n",
    "        downloaded += 1\n",
    "    except Exception as e:\n",
    "        errors.append((tsstr, str(e)))\n",
    "\n",
    "print(f\"Descargados: {downloaded} | Saltados: {skipped} | Errores: {len(errors)}\")\n",
    "if errors:\n",
    "    display(spark.createDataFrame(errors, schema=\"ts string, error string\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01b-bronze_gdelt_fetch_to_landing.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
